---
title: "PSTAT 131 Final Project"
author: "Karl Ma"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: show
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```
## Introduction
This project is used to create a model that can predict league of legend player's rank tires based on their performance in the game.

### What is rank tires in league of legend?

League of Legends has a ranking system called the League system, matching players of a similar skill level to play with and against each other. It comprises nine tiers which indicate the skill level of players. Here is how players divided into each tiers accord to League of Legend wiki Average player: Iron IV – Gold I (0–90 percentile), skilled player: Platinum IV – Diamond III(90–99.5 percentile),Elite player: Diamond II – Challenger (99.5–100 percentile).
```{r} 
library(vembedr)

embed_youtube("qoYPoL05ois")

```


### Description of my project 

In my project I collect player game play information from Gold Iv to Diamond I. I am going to create a model that can predict whether a player's performance in a game match their tier. The reason I am doing this project is that one of my friend player league of legend a lot and he think he deserve a better rank tier but I think he is at his skill level, therefore I am going to make some model to tell him that he is not good enough to be a diamond player.


## Loading Data and Packages

the code book is in the data file and here is some important variables.

laneMinionsFirst10Minutes: How many minions kill in the first 10 minutes in  the game this is a important variable to distinguish Elite player vs average player. Lane minions are the most reliable gold income source therefore, elite players do not miss any of them but average player does not focus on it.
visionScore: Another variable that distinguishes elite players from average players. This represent how much ward and deward a player did in a game, usually average player does not care vision much.

gameLength: How many seconds does the game last. The average game length is about 30 minutes but higher skill level the game trend to shorter. 

goldPerMinute: How much gold a player earn per minute.




```{r}
# load packages
library(tidyverse)
library(lubridate)
library(tidymodels)
library(patchwork)
library(janitor)
library(tidymodels)
library(ISLR)
library(tidyverse)
library(glmnet)
tidymodels_prefer()
library(janitor)
library(corrplot)
library(corrr)
library(rpart.plot)
library(vip)
library(randomForest)
library(xgboost)
```




```{r}
# load data
lol <- read.csv('data/datasetForR.csv')

lol <- lol %>% 
  select(-X, -index)
head(lol)
```



## Data Cleaning
Since I collect my data directly from riot most of the data cleaning part is done while I collect my data. refer to datacollect.py in the file 


select only games that been played more than 10 minutes. This step is to delete some variable that is not much useful. and conver some variable to factor
```{r}
set.seed(1101)
lol <- lol[lol$gameLength>600,]
lol <- lol %>% 
  select(-abilityUses, -alliedJungleMonsterKills,-completeSupportQuestInTime,
         -dragonTakedowns,-earlyLaningPhaseGoldExpAdvantage,-enemyJungleMonsterKills,-hadAfkTeammate,-immobilizeAndKillWithAlly,-landSkillShotsEarlyGame,-laningPhaseGoldExpAdvantage, -maxCsAdvantageOnLaneOpponent,-maxLevelLeadLaneOpponent,-skillshotsDodged,-skillshotsHit,-soloKills,-soloTurretsLategame, -takedownsFirst25Minutes,-visionScoreAdvantageLaneOpponent,-wardTakedownsBefore20M,-consumablesPurchased,-sightWardsBoughtInGame,-gameEndedInEarlySurrender)

lol <- lol %>% 
  mutate(gameEndedInSurrender = factor(gameEndedInSurrender),individualPosition = factor(individualPosition),win = factor(win),Tier = factor(Tier),Div = factor(Div))


```


## Data Split
split the data in 70%/30%
The training data set has 8090 observations
The testing data set has 3470 observations.
```{r}

set.seed(1101)
lol_split <- lol %>% 
  initial_split(prop = 0.7, strata =Tier )

lol_train <- training(lol_split)
lol_test <- testing(lol_split)


```

## Exploratory Data Analysis

This exploratory data analysis will use the training set, it has 8090 observations. Each observation represents a game that some player played.


My hypothesis is that Diamond players have more vision score accrose all positions. 
Lets plot some graphs use vision score and group by tiers and separate by different position since each position has different jobs.
```{r}
p <- ggplot(lol_train, aes(visionScorePerMinute,Tier))
p+geom_boxplot()+facet_wrap(vars(individualPosition))
```
As conclusion gold players has the least vision score per minute in all position, and diamond player has the highest score in all position. 


#### The next step I am going to explor the game length in different tiers.
This time the position does not matter so one graph is enough.
```{r}

p <- ggplot(lol_train, aes(gameLength,(Tier)))
p+geom_boxplot()

```

As conclusion gold player has the longest game length and diamond player has the least game length. 


#### Next lets see how well each tier player does in farming golds
first lets look at their first 10 minutes laneing phase.
since different position has different job in the game some position does not have to kill many minions therefore, show different position. 
```{r}
p <- ggplot(lol_train, aes(laneMinionsFirst10Minutes,(Tier)))
p+geom_boxplot()+facet_wrap(vars(individualPosition))


```
again there are obvious difference between different tiers. 

Then lets look at entier game gold income. 
```{r}
p <- ggplot(lol_train[lol_train$goldPerMinute<800,], aes(goldPerMinute,(Tier)))
p+geom_boxplot()+facet_wrap(vars(individualPosition))

```
In this plot we can tell that utility position does not have much difference between tiers that because this position does not need to farm gold they have different job in the team, in the other position there is some difference between tiers.  

#### Next lets look at how well each tier doing at fighting the enemy players

```{r}
p <- ggplot(lol_train[lol_train$damagePerMinute<2000,], aes(damagePerMinute,(Tier)))
p+geom_boxplot()+facet_wrap(vars(individualPosition))

```
This result tells us that different position in each tier has difference performance 


Kill participation is a variable of how often the player join his team. 
```{r}
p <- ggplot(lol_train[lol_train$killParticipation<1,], aes(killParticipation,(Tier)))
p+geom_boxplot()+facet_wrap(vars(individualPosition))

```
This plot tells us top is about the same in all tier but other position higher skill level the kill participation is also higher.



## Building Models

#### Recipe Building

fold the data into 5 folds
```{r}
set.seed(1101)
lol_folds <- vfold_cv(data = lol_train, v = 5, strata = Tier)

```

Create recipe

```{r}
set.seed(1101)
lol_recipe <- recipe(Tier ~ ., data = lol_train) %>% 
  # dummy encode categorical predictors
  step_dummy(all_nominal_predictors()) %>% 
  # center and scale all predictors
  step_normalize(all_predictors())

```



### Elastic Net
First setup the mode and engine of Elastic, tuning penalty and mixture.

```{r}
set.seed(1101)

elastic_net_spec <- multinom_reg(penalty = tune(), 
                                 mixture = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("glmnet")

```
Then create the workflow with recipe and model
```{r}
set.seed(1101)
elastic_workflow <- workflow() %>% 
  add_recipe(lol_recipe) %>% 
  add_model(elastic_net_spec)

```

Next set up the tuning grid set the penalty from -5 to 5 and mixture from 0 to 1 with 10 level each.
```{r}
set.seed(1101)
elastic_grid <- grid_regular(penalty(range = c(-5, 5)), 
                        mixture(range = c(0, 1)), levels = 10)

```

Execute the model
```{r}
set.seed(1101)
elastic_res <- tune_grid(
  elastic_workflow,
  resamples = lol_folds, 
  grid = elastic_grid
)



```



### Deccision Tree 

The process is similar, set the model and engine first, then setup the workflow with cost complexity tune.
when create the tune grid choose the range of cost complexity from -10 to -1 and 10 levels each. In the end execute the model.
```{r}
set.seed(1101)
# set the engine and model
tree_spec <- decision_tree() %>%
  set_engine("rpart")
class_tree_spec <- tree_spec %>%
  set_mode("classification")

# create the workflow
class_tree_wkflow <- workflow() %>% 
  add_model(class_tree_spec %>% 
              set_args(cost_complexity = tune())) %>%
  add_recipe(lol_recipe)

# tune grid
param_grid <- grid_regular(cost_complexity(range = c(-10, -1)), levels = 10)

# Execute the model
tree_tune_res <- tune_grid(
  class_tree_wkflow, 
  resamples = lol_folds, 
  grid = param_grid, 
  metrics = metric_set(roc_auc)
)

```

### Random Forest

The process is similar, set the model and engine first, then setup the workflow with mtry and trees tune.
when create the tune grid choose the range of mtry from 1 to 50, 50 is very close to my variable numbers, trees in range from 100 to 200, and 3 levels each. In the end execute the model.
```{r}
set.seed(1101)
# Setup engine and model
forest_spec <- rand_forest(mtry = .cols()) %>%
  set_engine("ranger", importance = 'impurity') %>%
  set_mode("classification")

# Create workflow
forest_wkflow <- workflow() %>% 
  add_model(forest_spec %>% 
              set_args(mtry = tune())%>% 
              set_args(trees = tune())) %>%
  add_recipe(lol_recipe)

```
```{r}
set.seed(1101)
# Create the tune grid
forest_grid <- grid_regular(mtry(range = c(1, 50)),trees(range=c(100,200)), levels = 3)

# Execute
forest_tune_res <- tune_grid(
  forest_wkflow, 
  resamples = lol_folds, 
  grid = forest_grid, 
  metrics = metric_set(roc_auc)
)

# save the result in rda.
save(forest_tune_res, forest_wkflow, file = "data/forest_.rda")
```



### Boost Tree

Repeat the similar process, set the model and engine first, then setup the workflow with mtry and trees tune.
when create the tune grid choose the range of mtry from 1 to 50, 50 is very close to my variable numbers, trees in range from 100 to 200, and 2 levels each. In the end execute the model.

```{r}
set.seed(1101)
# Setup engine and model
xgboost_spec <- boost_tree(trees = tune(),mtry=tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

# Create workflow
xgboost_wkflow <- workflow() %>% 
  add_model(xgboost_spec) %>%
  add_recipe(lol_recipe)

# Create the tune grid
xgboost_grid <- grid_regular(mtry=mtry(range(10,50)),trees(range=c(100,200)), levels = 2)

# Execute the model
xgboost_tune_res <- tune_grid(
  xgboost_wkflow, 
  resamples = lol_folds, 
  grid = xgboost_grid, 
  metrics = metric_set(roc_auc)
)
# save the result in rda.
save(xgboost_tune_res, xgboost_wkflow, file = "data/xgboost.rda")

```

## Model Analysis

show the best models

#### Elastic Net

Use the autoplot() function to visualize all models and use show_best() function to check the roc_auc value for this model. In this model the best roc_auc is 0.6308227 with n = 5 penalty = 0.00001 and mixture = 0.7777778 
```{r}

autoplot(elastic_res)
show_best(elastic_res, metric = "roc_auc")%>% select(-.estimator,-.config)

```
Smaller values of penalty tend to result in higher ROC-AUC.
now lets select the best model from this result.

```{r}
set.seed(1101)
# select the best model from elastic results.
best_elastic_model <- select_best(elastic_res, metric = "roc_auc")
# create a finalize workflow with the best model and elastic workflow.
elastic_final <- finalize_workflow(elastic_workflow, best_elastic_model)
# fit the model with training data
elastic_fit <- fit(elastic_final, data = lol_train)
# use the training data to create a prediction 
elastic_predicted_data <- augment(elastic_fit, new_data = lol_train) %>% 
  select(Tier, starts_with(".pred"))
# check the roc_auc value and plot the roc graph for all 3 tires.
elastic_predicted_data %>% roc_auc(Tier,.pred_DIAMOND,.pred_GOLD,.pred_PLATINUM)

elastic_predicted_data %>% roc_curve(Tier,.pred_DIAMOND,.pred_GOLD,.pred_PLATINUM) %>% 
  autoplot()

```

The best auc_roc of this model is 0.6496913 

### Deccision Tree 

Use the autoplot() function to visualize all models and use show_best() function to check the roc_auc value for this model. In this model the best roc_auc is 0.6403718 with n = 5  cost complexity = 0.001

```{r}
autoplot(tree_tune_res)

show_best(tree_tune_res, metric = "roc_auc")

```
select the best model from this result.
```{r}
set.seed(1101)
# select the best model from the tune results.
best_tree_model <- select_best(tree_tune_res, metric = "roc_auc")
# create the finalize workflow with deccision tree workflow and the best model.
tree_final <- finalize_workflow(class_tree_wkflow, best_tree_model)
# fit the model
tree_fit <- fit(tree_final, data = lol_train)
# use the training data to create a prediction 
tree_predicted_data <- augment(tree_fit, new_data = lol_train) %>% 
  select(Tier, starts_with(".pred"))
# check the roc_auc value and plot the roc graph for all 3 tires.
tree_predicted_data %>% roc_auc(Tier,.pred_DIAMOND,.pred_GOLD,.pred_PLATINUM)

tree_predicted_data %>% roc_curve(Tier,.pred_DIAMOND,.pred_GOLD,.pred_PLATINUM) %>% 
  autoplot()

```
The auc_roc of this model is 0.7726252 


### Random Forest

Use the autoplot() function to visualize all models and use show_best() function to check the roc_auc value for this model. In this model the best roc_auc is 0.7110287 with n = 5 trees = 200, and mtry = 50
```{r}
autoplot(forest_tune_res)
show_best(forest_tune_res, metric = "roc_auc")

```

```{r}
set.seed(1101)
# select the best model from the tune results.
best_forest_model <- select_best(forest_tune_res, metric = "roc_auc")
# create the finalize workflow with forest workflow and the best model
forest_final <- finalize_workflow(forest_wkflow, best_forest_model)
# fit the model with training data
forest_fit <- fit(forest_final, data = lol_train)
# use the training data to create a prediction 
forest_predicted_data <- augment(forest_fit, new_data = lol_train) %>% 
  select(Tier, starts_with(".pred"))


# check the roc_auc value and plot the roc graph for all 3 tires.
forest_predicted_data %>% roc_auc(Tier,.pred_DIAMOND,.pred_GOLD,.pred_PLATINUM)

forest_predicted_data %>% roc_curve(Tier,.pred_DIAMOND,.pred_GOLD,.pred_PLATINUM) %>% 
  autoplot()

```
This model is 0.9999997 auc_roc but that must be overfitting.


### Boost Tree

Use the autoplot() function to visualize all models and use show_best() function to check the roc_auc value for this model. In this model the best roc_auc is 0.7338508 with n = 5 trees = 200, and mtry = 50

```{r}
autoplot(xgboost_tune_res)
show_best(xgboost_tune_res, metric = "roc_auc")
```

```{r}
set.seed(1101)
# select the best model from the tune result.
best_xgboost_model <- select_best(xgboost_tune_res, metric = "roc_auc")
# create the finalize workflow with boost workflow and the best model
xgboost_final <- finalize_workflow(xgboost_wkflow, best_xgboost_model)

# fit the model with training data
xgboost_fit <- fit(xgboost_final, data = lol_train)

# use the training data to create a prediction 
xgboost_predicted_data <- augment(xgboost_fit, new_data = lol_train) %>% 
  select(Tier, starts_with(".pred"))

# check the roc_auc value and plot the roc graph for all 3 tires.
xgboost_predicted_data %>% roc_auc(Tier,.pred_DIAMOND,.pred_GOLD,.pred_PLATINUM)

xgboost_predicted_data %>% roc_curve(Tier,.pred_DIAMOND,.pred_GOLD,.pred_PLATINUM) %>% 
  autoplot()

```

This model is the best has roc_auc value 1 I am pretty sure there is a lot  overfitting happend in this model.


### Build the Final Model 
Since boost tree perform the best with our training data I am going to use it for the final model building.
To do this we can repeat our process above but this time when we create predicted data we use our testing dataset. 
```{r}
set.seed(1101)
# select the best model from boost tree tune result.
best_xgboost_model <- select_best(xgboost_tune_res, metric = "roc_auc")

# create finalize workflow with boost tree workflow and best model.
xgboost_final <- finalize_workflow(xgboost_wkflow, best_xgboost_model)
# fit the model with training data
xgboost_fit <- fit(xgboost_final, data = lol_train)
# create predicted data with the test dataset
xgboost_predicted_data <- augment(xgboost_fit, new_data = lol_test) %>% 
  select(Tier, starts_with(".pred"))

# check the roc_auc value and plot the roc graph for all 3 tires.
xgboost_predicted_data %>% roc_auc(Tier,.pred_DIAMOND,.pred_GOLD,.pred_PLATINUM)

xgboost_predicted_data %>% roc_curve(Tier,.pred_DIAMOND,.pred_GOLD,.pred_PLATINUM) %>% 
  autoplot()

# create a heat map to have a better visualization of the model.
augment(xgboost_fit, new_data = lol_test) %>% conf_mat(truth=Tier,estimate=.pred_class)%>%autoplot(type='heatmap')
```




## Conclusion 
After testing 4 different models, I decided use the Boost Tree to do the final model building. The final model has roc_auc value 0.7596927. The model is best at predict Diamond player and did worst at predict Platinum player
For further improvement of this project I could take challenger, silver and bronze player into the dataset, I did not do it this time because the api does not have challenger as an option and silver and bronze players have too many random noise in their data. I should also separate the positions to create different models because different position in all tiers have huge difference game performance, also I am thinking about change some of the tune value and levels if I have more time to run the model.











---
title: "PSTAT 131 Final Project"
author: "Karl Ma"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: show
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```
## Introduction
This project is used to create a model that can predict league of legend player's rank tires based on their performance in the game.

### What is rank tires in league of legend?

League of Legends has a ranking system called the League system, matching players of a similar skill level to play with and against each other. It comprises nine tiers which indicate the skill level of players. Here is how players divided into each tiers accord to League of Legend wiki Average player: Iron IV – Gold I (0–90 percentile), skilled player: Platinum IV – Diamond III(90–99.5 percentile),Elite player: Diamond II – Challenger (99.5–100 percentile).
```{r} 
library(vembedr)

embed_youtube("qoYPoL05ois")

```


### Description of my project 

In my project I collect player game play information from Gold Iv to Diamond I. I am going to create a model that can predict whether a player's performance in a game match their tier. The reason I am doing this project is that one of my friend player league of legend a lot and he think he deserve a better rank tier but I think he is at his skill level, therefore I am going to make some model to tell him that he is not good enough to be a diamond player.


## Loading Data and Packages

the code book is in the data file and here is some important variables.

laneMinionsFirst10Minutes: How many minions kill in the first 10 minutes in  the game this is a important variable to distinguish Elite player vs average player. Lane minions are the most reliable gold income source therefore, elite players do not miss any of them but average player does not focus on it.
visionScore: Another variable that distinguishes elite players from average players. This represent how much ward and deward a player did in a game, usually average player does not care vision much.

gameLength: How many seconds does the game last. The average game length is about 30 minutes but higher skill level the game trend to shorter. 

goldPerMinute: How much gold a player earn per minute.




```{r}
# load packages
library(tidyverse)
library(lubridate)
library(tidymodels)
library(patchwork)
library(janitor)
library(tidymodels)
library(ISLR)
library(tidyverse)
library(glmnet)
tidymodels_prefer()
library(janitor)
library(corrplot)
library(corrr)
library(rpart.plot)
library(vip)
library(randomForest)
library(xgboost)
```




```{r}
# load data
lol <- read.csv('data/datasetForR.csv')

lol <- lol %>% 
  select(-X, -index)
head(lol)
```



## Data Cleaning
Since I collect my data directly from riot most of the data cleaning part is done while I collect my data. refer to datacollect.py in the file 


select only games that been played more than 10 minutes. This step is to delete some variable that is not much useful. and conver some variable to factor
```{r}
set.seed(1101)
lol <- lol[lol$gameLength>600,]
lol <- lol %>% 
  select(-abilityUses, -alliedJungleMonsterKills,-completeSupportQuestInTime,
         -dragonTakedowns,-earlyLaningPhaseGoldExpAdvantage,-enemyJungleMonsterKills,-hadAfkTeammate,-immobilizeAndKillWithAlly,-landSkillShotsEarlyGame,-laningPhaseGoldExpAdvantage, -maxCsAdvantageOnLaneOpponent,-maxLevelLeadLaneOpponent,-skillshotsDodged,-skillshotsHit,-soloKills,-soloTurretsLategame, -takedownsFirst25Minutes,-visionScoreAdvantageLaneOpponent,-wardTakedownsBefore20M,-consumablesPurchased,-sightWardsBoughtInGame,-gameEndedInEarlySurrender)

lol <- lol %>% 
  mutate(gameEndedInSurrender = factor(gameEndedInSurrender),individualPosition = factor(individualPosition),win = factor(win),Tier = factor(Tier),Div = factor(Div))


```


## Data Split
split the data in 70%/30%
The training data set has 8090 observations
The testing data set has 3470 observations.
```{r}

set.seed(1101)
lol_split <- lol %>% 
  initial_split(prop = 0.7, strata =Tier )

lol_train <- training(lol_split)
lol_test <- testing(lol_split)


```

## Exploratory Data Analysis

This exploratory data analysis will use the training set, it has 8090 observations. Each observation represents a game that some player played.


####My hypothesis is that Diamond players have more vision score accrose all positions. 
Lets plot some graphs use vision score and group by tiers and separate by different position since each position has different jobs.
```{r}
p <- ggplot(lol_train, aes(visionScorePerMinute,Tier))
p+geom_boxplot()+facet_wrap(vars(individualPosition))
```
As conclusion gold players has the least vision score per minute in all position, and diamond player has the highest score in all position. 


#### The next step I am going to explor the game length in different tiers.
This time the position does not matter so one graph is enough.
```{r}

p <- ggplot(lol_train, aes(gameLength,(Tier)))
p+geom_boxplot()

```

As conclusion gold player has the longest game length and diamond player has the least game length. 


#### Next lets see how well each tier player does in farming golds
first lets look at their first 10 minutes laneing phase.
since different position has different job in the game some position does not have to kill many minions therefore, show different position. 
```{r}
p <- ggplot(lol_train, aes(laneMinionsFirst10Minutes,(Tier)))
p+geom_boxplot()+facet_wrap(vars(individualPosition))


```
again there are obvious difference between different tiers. 

Then lets look at entier game gold income. 
```{r}
p <- ggplot(lol_train[lol_train$goldPerMinute<800,], aes(goldPerMinute,(Tier)))
p+geom_boxplot()+facet_wrap(vars(individualPosition))

```
In this plot we can tell that utility position does not have much difference between tiers that because this position does not need to farm gold they have different job in the team, in the other position there is some difference between tiers.  

#### Next lets look at how well each tier doing at fighting the enemy players

```{r}
p <- ggplot(lol_train[lol_train$damagePerMinute<2000,], aes(damagePerMinute,(Tier)))
p+geom_boxplot()+facet_wrap(vars(individualPosition))

```
This result tells us that different position in each tier has difference performance 


Kill participation is a variable of how often the player join his team. 
```{r}
p <- ggplot(lol_train[lol_train$killParticipation<1,], aes(killParticipation,(Tier)))
p+geom_boxplot()+facet_wrap(vars(individualPosition))

```
This plot tells us top is about the same in all tier but other position higher skill level the kill participation is also higher.



## Building Models

#### Recipe Building

fold the data
```{r}
set.seed(1101)
lol_folds <- vfold_cv(data = lol_train, v = 5, strata = Tier)

```

Create recipe

```{r}
set.seed(1101)
lol_recipe <- recipe(Tier ~ ., data = lol_train) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_predictors())

```



### Elastic Net Tuning
```{r}
set.seed(1101)
elastic_net_spec <- multinom_reg(penalty = tune(), 
                                 mixture = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("glmnet")

```
```{r}
set.seed(1101)
elastic_workflow <- workflow() %>% 
  add_recipe(lol_recipe) %>% 
  add_model(elastic_net_spec)

```


```{r}
set.seed(1101)
elastic_grid <- grid_regular(penalty(range = c(-5, 5)), 
                        mixture(range = c(0, 1)), levels = 10)

```

```{r}
set.seed(1101)
elastic_res <- tune_grid(
  elastic_workflow,
  resamples = lol_folds, 
  grid = elastic_grid
)



```



### Deccision tree 

```{r}
set.seed(1101)
tree_spec <- decision_tree() %>%
  set_engine("rpart")
class_tree_spec <- tree_spec %>%
  set_mode("classification")

class_tree_wkflow <- workflow() %>% 
  add_model(class_tree_spec %>% 
              set_args(cost_complexity = tune())) %>%
  add_recipe(lol_recipe)


param_grid <- grid_regular(cost_complexity(range = c(-10, -1)), levels = 10)

tree_tune_res <- tune_grid(
  class_tree_wkflow, 
  resamples = lol_folds, 
  grid = param_grid, 
  metrics = metric_set(roc_auc)
)

```
```{r}
set.seed(1101)
show_best(tree_tune_res, metric = "roc_auc")

```
### random forest
```{r}
set.seed(1101)
forest_spec <- rand_forest(mtry = .cols()) %>%
  set_engine("ranger", importance = 'impurity') %>%
  set_mode("classification")

forest_wkflow <- workflow() %>% 
  add_model(forest_spec %>% 
              set_args(mtry = tune())%>% 
              set_args(trees = tune())) %>%
  add_recipe(lol_recipe)

```
```{r}
set.seed(1101)
forest_grid <- grid_regular(mtry(range = c(1, 50)),trees(range=c(100,200)), levels = 3)

forest_tune_res <- tune_grid(
  forest_wkflow, 
  resamples = lol_folds, 
  grid = forest_grid, 
  metrics = metric_set(roc_auc)
)


save(forest_tune_res, forest_wkflow, file = "data/forest_.rda")
```



### boost tree
```{r}
set.seed(1101)
xgboost_spec <- boost_tree(trees = tune(),mtry=tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification")


xgboost_wkflow <- workflow() %>% 
  add_model(xgboost_spec) %>%
  add_recipe(lol_recipe)


xgboost_grid <- grid_regular(mtry=mtry(range(10,50)),trees(range=c(100,200)), levels = 2)

xgboost_tune_res <- tune_grid(
  xgboost_wkflow, 
  resamples = lol_folds, 
  grid = xgboost_grid, 
  metrics = metric_set(roc_auc)
)

save(xgboost_tune_res, xgboost_wkflow, file = "data/xgboost.rda")

```

## Model Analysis

show the best models

#### Elastic Net
```{r}

autoplot(elastic_res)
show_best(elastic_res, metric = "roc_auc")%>% select(-.estimator,-.config)

```
Smaller values of penalty tend to result in higher ROC-AUC.
now lets select the best model from this result.
```{r}
set.seed(1101)
best_elastic_model <- select_best(elastic_res, metric = "roc_auc")
elastic_final <- finalize_workflow(elastic_workflow, best_elastic_model)
elastic_fit <- fit(elastic_final, data = lol_train)

elastic_predicted_data <- augment(elastic_fit, new_data = lol_train) %>% 
  select(Tier, starts_with(".pred"))

elastic_predicted_data %>% roc_auc(Tier,.pred_DIAMOND,.pred_GOLD,.pred_PLATINUM)

elastic_predicted_data %>% roc_curve(Tier,.pred_DIAMOND,.pred_GOLD,.pred_PLATINUM) %>% 
  autoplot()

```

The auc_roc of this model is 0.6482557 

### Deccision tree 

```{r}
autoplot(tree_tune_res)

show_best(tree_tune_res, metric = "roc_auc")

```

```{r}
set.seed(1101)
best_tree_model <- select_best(tree_tune_res, metric = "roc_auc")
tree_final <- finalize_workflow(class_tree_wkflow, best_tree_model)
tree_fit <- fit(tree_final, data = lol_train)

tree_predicted_data <- augment(tree_fit, new_data = lol_train) %>% 
  select(Tier, starts_with(".pred"))

tree_predicted_data %>% roc_auc(Tier,.pred_DIAMOND,.pred_GOLD,.pred_PLATINUM)

tree_predicted_data %>% roc_curve(Tier,.pred_DIAMOND,.pred_GOLD,.pred_PLATINUM) %>% 
  autoplot()

```
The auc_roc of this model is 0.7648482 


### random forest


```{r}
autoplot(forest_tune_res)
show_best(forest_tune_res, metric = "roc_auc")

```
```{r}
set.seed(1101)
best_forest_model <- select_best(forest_tune_res, metric = "roc_auc")
forest_final <- finalize_workflow(forest_wkflow, best_forest_model)
forest_fit <- fit(forest_final, data = lol_train)

forest_predicted_data <- augment(forest_fit, new_data = lol_train) %>% 
  select(Tier, starts_with(".pred"))

forest_predicted_data %>% roc_auc(Tier,.pred_DIAMOND,.pred_GOLD,.pred_PLATINUM)

forest_predicted_data %>% roc_curve(Tier,.pred_DIAMOND,.pred_GOLD,.pred_PLATINUM) %>% 
  autoplot()

```
This model is the best has 0.9999994 but that must be overfitting.






```{r}
autoplot(xgboost_tune_res)
show_best(xgboost_tune_res, metric = "roc_auc")
```

```{r}
set.seed(1101)
best_xgboost_model <- select_best(xgboost_tune_res, metric = "roc_auc")
xgboost_final <- finalize_workflow(xgboost_wkflow, best_xgboost_model)
xgboost_fit <- fit(xgboost_final, data = lol_train)

xgboost_predicted_data <- augment(xgboost_fit, new_data = lol_train) %>% 
  select(Tier, starts_with(".pred"))

xgboost_predicted_data %>% roc_auc(Tier,.pred_DIAMOND,.pred_GOLD,.pred_PLATINUM)

xgboost_predicted_data %>% roc_curve(Tier,.pred_DIAMOND,.pred_GOLD,.pred_PLATINUM) %>% 
  autoplot()

```

This model has roc_auc value 1 I am pretty sure there is overfitting



```{r}
set.seed(1101)
best_xgboost_model <- select_best(xgboost_tune_res, metric = "roc_auc")
xgboost_final <- finalize_workflow(xgboost_wkflow, best_xgboost_model)
xgboost_fit <- fit(xgboost_final, data = lol_train)

xgboost_predicted_data <- augment(xgboost_fit, new_data = lol_test) %>% 
  select(Tier, starts_with(".pred"))

xgboost_predicted_data %>% roc_auc(Tier,.pred_DIAMOND,.pred_GOLD,.pred_PLATINUM)

xgboost_predicted_data %>% roc_curve(Tier,.pred_DIAMOND,.pred_GOLD,.pred_PLATINUM) %>% 
  autoplot()
augment(xgboost_fit, new_data = lol_test) %>% conf_mat(truth=Tier,estimate=.pred_class)%>%autoplot(type='heatmap')
```

The final model is about 76.12% accurate. The model is best at predict Diamond player and did worst at predict Platinum player